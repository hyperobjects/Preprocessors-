{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> This notebook demonstrates how to preprocess a text corpus using NLKT, spaCy, Stanford CoreNLP, and Spark NLP.</h1>\n",
    "\n",
    "For a breakdown of features for the libraries showcased in this notebook, check out: https://blog.dominodatalab.com/comparing-the-functionality-of-open-source-natural-language-processing-libraries/ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>The first thing to do is load a corpus of text documents from a local directory. In this example, the corpus is a collection of 11 speeches by Dr. Martin Luther King.<h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "docs = []\n",
    "for filename in glob.glob('./King/*.txt'): \n",
    "    with open(filename, 'r', encoding='utf-8') as f:     \n",
    "        docs.append(f.read().replace('\\n', ' '))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Now preprocess with NLTK --a powerful NLP library commonly used in research and academia. </h4>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english')) \n",
    "\n",
    "cleanDocs = []\n",
    "\n",
    "# the tag_switch function allows one to switch the \n",
    "# pos tags from the format used by nltk.pos_tag \n",
    "# to one that is recognizable by WordNetLemmatizer \n",
    "# so all word tokens can be lemmatized. \n",
    "\n",
    "def tag_switch(word):\n",
    "    \n",
    "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
    "    tags = {'J': wordnet.ADJ, 'N': wordnet.NOUN,\n",
    "            'V': wordnet.VERB, 'R': wordnet.ADV}\n",
    "    return tags.get(tag, wordnet.NOUN)\n",
    "\n",
    "# the code for tag_switch was taken from the nltk examples at the url below\n",
    "# https://www.machinelearningplus.com/nlp/lemmatization-examples-python/\n",
    "\n",
    "def nltk_preprocess(corpus): \n",
    "    for doc in corpus:       \n",
    "        (cleanDocs.append([lemmatizer.lemmatize(word.lower(), tag_switch(word)) for word \n",
    "                           in nltk.word_tokenize(doc) if word.isalpha() and word.lower() \n",
    "                           not in stop_words]))\n",
    "    #return cleanDocs  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk_preprocess(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>To get a feel for how the corpus has been transformed, let's take look at the first 100 tokens for the first text in cleanDocs.</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['happy', 'join', 'today', 'go', 'history', 'great', 'demonstration', 'freedom', 'history', 'nation', 'five', 'score', 'year', 'ago', 'great', 'american', 'whose', 'symbolic', 'shadow', 'stand', 'today', 'sign', 'emancipation', 'proclamation', 'momentous', 'decree', 'come', 'great', 'beacon', 'light', 'hope', 'million', 'negro', 'slave', 'sear', 'flame', 'wither', 'injustice', 'come', 'joyous', 'daybreak', 'end', 'long', 'night', 'captivity', 'one', 'hundred', 'year', 'later', 'negro', 'still', 'free', 'one', 'hundred', 'year', 'later', 'life', 'negro', 'still', 'sadly', 'cripple', 'manacle', 'segregation', 'chain', 'discrimination', 'one', 'hundred', 'year', 'later', 'negro', 'life', 'lonely', 'island', 'poverty', 'midst', 'vast', 'ocean', 'material', 'prosperity', 'one', 'hundred', 'year', 'later', 'negro', 'still', 'languish', 'corner', 'american', 'society', 'find', 'exile', 'land', 'come', 'today', 'dramatize', 'shameful', 'condition', 'sense', 'come', 'nation']\n"
     ]
    }
   ],
   "source": [
    "print(cleanDocs[0][:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Now reload the raw corpus.</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "docs = []\n",
    "for filename in glob.glob('./King/*.txt'):  \n",
    "    with open(filename, 'r', encoding='utf-8') as f:     \n",
    "        docs.append(f.read().replace('\\n', ' '))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Next preprocess the docs with spaCy --an industry standard library which creates an integrated doc object and provides easy access to linguistic annotations.</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "cleanDocs = []\n",
    "\n",
    "def spacy_preprocess(corpus): \n",
    "    for doc in range(len(corpus)):\n",
    "        corpus[doc] = nlp(corpus[doc]) \n",
    "        (cleanDocs.append([word.lemma_.lower() for word in corpus[doc] \n",
    "                           if word.is_stop ==False and word.lemma_.isalpha()]))\n",
    "    \n",
    "    #return cleanDocs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy_preprocess(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>It is always good practice to take a look at the processed data to make sure the results are as expected before going further in a project or experiment. A glance at the first 100 tokens in the second doc shows that spaCy didn't lemmatize everything properly.</h4>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['order', 'answer', 'question', 'theme', 'honestly', 'recognize', 'constitution', 'write', 'strange', 'formula', 'determine', 'taxis', 'representation', 'declare', 'negro', 'percent', 'person', 'today', 'curious', 'formula', 'declare', 'percent', 'person', 'good', 'thing', 'life', 'negro', 'approximately', 'half', 'white', 'bad', 'thing', 'life', 'twice', 'white', 'half', 'negroes', 'live', 'substandard', 'housing', 'negroes', 'half', 'income', 'white', 'view', 'negative', 'experience', 'life', 'negro', 'double', 'share', 'twice', 'unemployed', 'rate', 'infant', 'mortality', 'negroes', 'double', 'white', 'twice', 'negroes', 'die', 'vietnam', 'white', 'proportion', 'size', 'population', 'sphere', 'figure', 'equally', 'alarming', 'elementary', 'school', 'negroes', 'lag', 'year', 'white', 'segregated', 'school', 'receive', 'substantially', 'money', 'student', 'white', 'school', 'twentieth', 'negroes', 'white', 'attend', 'college', 'employed', 'negroes', 'percent', 'hold', 'menial', 'job', 'massively', 'assert', 'dignity', 'worth']\n"
     ]
    }
   ],
   "source": [
    "print(cleanDocs[1][:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Let's take a look at tokens which end in 's' in the second document to figure out why all plural nouns haven't been lemmatized with spaCy.</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VERB is be\n",
      "VERB was be\n",
      "NOUN taxes taxis\n",
      "VERB was be\n",
      "ADJ curious curious\n",
      "VERB seems seem\n",
      "VERB is be\n",
      "NOUN things thing\n",
      "VERB has have\n",
      "NOUN whites white\n",
      "NOUN things thing\n",
      "VERB has have\n",
      "NOUN whites white\n",
      "ADV thus thus\n",
      "PROPN negroes Negroes\n",
      "PROPN negroes Negroes\n",
      "NOUN whites white\n",
      "NOUN experiences experience\n",
      "VERB has have\n",
      "ADV as as\n",
      "PROPN negroes Negroes\n",
      "VERB is be\n",
      "NOUN whites white\n",
      "ADV as as\n",
      "PROPN negroes Negroes\n",
      "ADP as as\n",
      "NOUN whites white\n",
      "NOUN spheres sphere\n",
      "NOUN figures figure\n",
      "NOUN schools school\n",
      "PROPN negroes Negroes\n",
      "NOUN years year\n",
      "NOUN whites white\n",
      "NOUN schools school\n",
      "ADJ less less\n",
      "NOUN schools school\n",
      "ADP as as\n",
      "PROPN negroes Negroes\n",
      "ADP as as\n",
      "NOUN whites white\n",
      "PROPN negroes Negroes\n",
      "NOUN jobs job\n",
      "DET this this\n",
      "VERB is be\n",
      "VERB oppresses oppress\n",
      "PRON us -PRON-\n",
      "NOUN values value\n",
      "NOUN centuries century\n",
      "VERB is be\n",
      "PROPN blackness Blackness\n",
      "PROPN contributions Contributions\n",
      "NOUN semantics semantic\n",
      "VERB is be\n",
      "PART 's 's\n",
      "PROPN thesaurus Thesaurus\n",
      "NOUN synonyms synonym\n",
      "NOUN blackness blackness\n",
      "ADP as as\n",
      "NOUN synonyms synonyms\n",
      "NOUN whiteness whiteness\n",
      "NOUN words word\n",
      "ADP as as\n",
      "NOUN cleanliness cleanliness\n",
      "VERB is be\n",
      "VERB is be\n",
      "PROPN davis Davis\n",
      "VERB has have\n",
      "NOUN teachers teacher\n",
      "NOUN ways way\n",
      "DET his -PRON-\n",
      "NOUN ways way\n",
      "DET his -PRON-\n",
      "PART 's 's\n",
      "DET his -PRON-\n",
      "VERB is be\n",
      "ADV as as\n",
      "ADP as as\n",
      "NOUN hooks hook\n",
      "ADV as as\n",
      "ADP as as\n",
      "PART 's 's\n",
      "DET this this\n",
      "DET his -PRON-\n",
      "PART 's 's\n",
      "VERB overlooks overlook\n",
      "DET this this\n",
      "VERB is be\n",
      "ADV as as\n",
      "ADP as as\n",
      "VERB is be\n",
      "VERB is be\n",
      "PROPN rights Rights\n",
      "DET this this\n",
      "VERB reaches reach\n",
      "NOUN depths depth\n",
      "DET his -PRON-\n",
      "NOUN signs sign\n",
      "DET his -PRON-\n",
      "NOUN manacles manacle\n",
      "VERB has have\n",
      "INTJ yes yes\n",
      "VERB was be\n",
      "NOUN foreparents foreparent\n",
      "INTJ yes yes\n",
      "DET this this\n",
      "VERB is be\n",
      "PART 's 's\n",
      "PART 's 's\n",
      "NOUN crimes crime\n",
      "PROPN challenges Challenges\n",
      "VERB is be\n",
      "NOUN terms term\n",
      "VERB is be\n",
      "DET this this\n",
      "NOUN problems problem\n",
      "NOUN confronts confront\n",
      "VERB is be\n",
      "DET his -PRON-\n",
      "NOUN plantations plantation\n",
      "NOUN ghettos ghetto\n",
      "VERB has have\n",
      "NOUN voicelessness voicelessness\n",
      "NOUN powerlessness powerlessness\n",
      "NOUN decisions decision\n",
      "DET his -PRON-\n",
      "VERB has have\n",
      "ADV sometimes sometimes\n",
      "NOUN decisions decision\n",
      "DET this this\n",
      "NOUN powerlessness powerlessness\n",
      "VERB is be\n",
      "NOUN forces force\n",
      "NOUN forces force\n",
      "NOUN status status\n",
      "VERB is be\n",
      "VERB is be\n",
      "VERB is be\n",
      "PROPN motors Motors\n",
      "INTJ yes yes\n",
      "VERB wants want\n",
      "VERB 's be\n",
      "PRON us -PRON-\n",
      "NOUN preachers preacher\n",
      "PRON us -PRON-\n",
      "NOUN convictions conviction\n",
      "NOUN concerns concern\n",
      "NOUN problems problem\n",
      "VERB is be\n",
      "VERB is be\n",
      "VERB is be\n",
      "NOUN philosophers philosopher\n",
      "NOUN problems problem\n",
      "VERB is be\n",
      "NOUN concepts concept\n",
      "ADP as as\n",
      "NOUN opposites opposite\n",
      "VERB is be\n",
      "VERB was be\n",
      "DET this this\n",
      "VERB was be\n",
      "VERB was be\n",
      "DET this this\n",
      "NOUN theologians theologian\n",
      "DET this this\n",
      "VERB is be\n",
      "VERB is be\n",
      "VERB is be\n",
      "ADJ reckless reckless\n",
      "VERB is be\n",
      "DET its -PRON-\n",
      "VERB is be\n",
      "NOUN demands demand\n",
      "DET its -PRON-\n",
      "VERB is be\n",
      "VERB stands stand\n",
      "DET this this\n",
      "VERB is be\n",
      "ADP as as\n",
      "VERB has have\n",
      "VERB is be\n",
      "DET this this\n",
      "VERB has have\n",
      "PROPN americans Americans\n",
      "NOUN goals goal\n",
      "DET this this\n",
      "VERB is be\n",
      "NOUN extremists extremist\n",
      "PROPN negroes Negroes\n",
      "ADJ conscienceless conscienceless\n",
      "NOUN whites white\n",
      "VERB is be\n",
      "DET this this\n",
      "ADJ powerless powerless\n",
      "VERB constitutes constitute\n",
      "NOUN crisis crisis\n",
      "NOUN times time\n",
      "DET this this\n",
      "DET this this\n",
      "ADP as as\n",
      "NOUN status status\n",
      "VERB was be\n",
      "PART 's 's\n",
      "NOUN talents talent\n",
      "NOUN goods good\n",
      "ADJ industrious industrious\n",
      "NOUN habits habit\n",
      "NOUN dislocations dislocation\n",
      "NOUN operations operation\n",
      "NOUN idleness idleness\n",
      "ADV less less\n",
      "NOUN consciences conscience\n",
      "ADV as as\n",
      "VERB develops develop\n",
      "VERB expands expand\n",
      "VERB does do\n",
      "VERB indicates indicate\n",
      "NOUN emphasis emphasis\n",
      "NOUN incomes income\n",
      "NOUN consumers consumer\n",
      "DET this this\n",
      "VERB is be\n",
      "NOUN forms form\n",
      "NOUN jobs job\n",
      "DET this this\n",
      "NOUN affairs affair\n",
      "PROPN progress Progress\n",
      "VERB is be\n",
      "VERB improves improve\n",
      "VERB extends extend\n",
      "VERB increases increase\n",
      "NOUN enriches enriche\n",
      "NOUN elevates elevate\n",
      "VERB is be\n",
      "VERB is be\n",
      "NOUN slaves slave\n",
      "NOUN tasks task\n",
      "VERB is be\n",
      "VERB brings bring\n",
      "DET its -PRON-\n",
      "VERB is be\n",
      "DET this this\n",
      "NOUN problems problem\n",
      "PRON themselves -PRON-\n",
      "VERB is be\n",
      "NOUN purchasers purchaser\n",
      "PROPN negroes Negroes\n",
      "NOUN advantages advantage\n",
      "NOUN changes change\n",
      "NOUN decisions decision\n",
      "DET his -PRON-\n",
      "DET his -PRON-\n",
      "NOUN hands hand\n",
      "VERB has have\n",
      "NOUN means mean\n",
      "NOUN conflicts conflict\n",
      "NOUN husbands husband\n",
      "NOUN wives wife\n",
      "NOUN dollars dollar\n",
      "VERB is be\n",
      "DET this this\n",
      "NOUN dollars dollar\n",
      "NOUN dollars dollar\n",
      "NOUN dollars dollar\n",
      "NOUN billions billion\n",
      "NOUN dollars dollar\n",
      "PART 's 's\n",
      "VERB stress stress\n",
      "DET this this\n",
      "VERB has have\n",
      "NOUN riots riot\n",
      "NOUN riots riot\n",
      "NOUN causes cause\n",
      "VERB is be\n",
      "VERB sees see\n",
      "NOUN youngsters youngster\n",
      "NOUN adults adult\n",
      "NOUN odds odd\n",
      "PROPN negroes Negroes\n",
      "PROPN watts Watts\n",
      "NOUN riots riot\n",
      "ADJ various various\n",
      "NOUN cities city\n",
      "NOUN rights right\n",
      "VERB express express\n",
      "DET this this\n",
      "ADV always always\n",
      "NOUN words word\n",
      "NOUN gains gain\n",
      "ADP as as\n",
      "NOUN riots riot\n",
      "NOUN officials official\n",
      "NOUN sprinklers sprinkler\n",
      "NOUN ghettos ghetto\n",
      "VERB is be\n",
      "NOUN bars bar\n",
      "NOUN riots riot\n",
      "ADP as as\n",
      "NOUN demonstrations demonstration\n",
      "VERB tries try\n",
      "NOUN advocates advocate\n",
      "ADP as as\n",
      "NOUN acts act\n",
      "NOUN answers answer\n",
      "ADV sometimes sometimes\n",
      "NOUN governments government\n",
      "VERB has have\n",
      "ADP unless unless\n",
      "DET its -PRON-\n",
      "NOUN forces force\n",
      "DET his -PRON-\n",
      "VERB knows know\n",
      "DET this this\n",
      "PROPN states States\n",
      "VERB has have\n",
      "NOUN troopers trooper\n",
      "NOUN revolutions revolution\n",
      "ADP unless unless\n",
      "PROPN cubans Cubans\n",
      "NOUN hills hill\n",
      "ADP unless unless\n",
      "VERB is be\n",
      "NOUN blacks black\n",
      "PROPN negroes Negroes\n",
      "PRON themselves -PRON-\n",
      "DET this this\n",
      "VERB is be\n",
      "NOUN illusions illusion\n",
      "NOUN debates debate\n",
      "DET this this\n",
      "VERB is be\n",
      "VERB is be\n",
      "VERB is be\n",
      "ADV as as\n",
      "ADP as as\n",
      "DET this this\n",
      "VERB has have\n",
      "DET this this\n",
      "NOUN solutions solution\n",
      "NOUN answers answer\n",
      "NOUN explanations explanation\n",
      "VERB is be\n",
      "DET his -PRON-\n",
      "DET this this\n",
      "VERB is be\n",
      "VERB is be\n",
      "NOUN darkness darkness\n",
      "NOUN darkness darkness\n",
      "VERB is be\n",
      "PART 's 's\n",
      "NOUN problems problem\n",
      "VERB is be\n",
      "NOUN circles circle\n",
      "NOUN faces face\n",
      "NOUN sheriffs sheriff\n",
      "NOUN faces face\n",
      "PROPN citizens Citizens\n",
      "PROPN councilors Councilors\n",
      "VERB does do\n",
      "NOUN faces face\n",
      "NOUN personalities personality\n",
      "VERB is be\n",
      "VERB is be\n",
      "VERB was be\n",
      "VERB is be\n",
      "VERB hates hate\n",
      "VERB does do\n",
      "VERB has have\n",
      "VERB has have\n",
      "VERB unlocks unlock\n",
      "ADP as as\n",
      "ADP as as\n",
      "VERB address address\n",
      "NOUN questions question\n",
      "NOUN questions question\n",
      "NOUN beggars beggar\n",
      "PART 's 's\n",
      "VERB produces produce\n",
      "NOUN beggars beggar\n",
      "VERB needs need\n",
      "VERB means mean\n",
      "NOUN questions question\n",
      "NOUN friends friend\n",
      "DET this this\n",
      "VERB owns own\n",
      "VERB owns own\n",
      "VERB is be\n",
      "NOUN bills bill\n",
      "VERB is be\n",
      "NOUN thirds third\n",
      "NOUN questions question\n",
      "DET this this\n",
      "VERB is be\n",
      "VERB forgets forget\n",
      "VERB is be\n",
      "VERB forgets forget\n",
      "VERB is be\n",
      "VERB is be\n",
      "NOUN thesis thesis\n",
      "NOUN antithesis antithesis\n",
      "NOUN synthesis synthesis\n",
      "VERB is be\n",
      "NOUN synthesis synthesis\n",
      "VERB combines combine\n",
      "NOUN truths truth\n",
      "VERB means mean\n",
      "NOUN evils evil\n",
      "PROPN jesus Jesus\n",
      "PROPN jesus Jesus\n",
      "PROPN jesus Jesus\n",
      "PROPN nicodemus Nicodemus\n",
      "PROPN nicodemus Nicodemus\n",
      "PROPN nicodemus Nicodemus\n",
      "PROPN nicodemus Nicodemus\n",
      "PROPN jesus Jesus\n",
      "PROPN jesus Jesus\n",
      "PROPN nicodemus Nicodemus\n",
      "NOUN words word\n",
      "NOUN years year\n",
      "NOUN things thing\n",
      "NOUN investments investment\n",
      "DET its -PRON-\n",
      "NOUN problems problem\n",
      "VERB is be\n",
      "DET this this\n",
      "PRON us -PRON-\n",
      "PRON us -PRON-\n",
      "NOUN creeds creed\n",
      "NOUN deeds deed\n",
      "PRON us -PRON-\n",
      "NOUN walls wall\n",
      "NOUN rams ram\n",
      "NOUN forces force\n",
      "PRON us -PRON-\n",
      "NOUN outskirts outskirt\n",
      "NOUN metropolis metropolis\n",
      "PRON us -PRON-\n",
      "NOUN slums slum\n",
      "NOUN heaps heap\n",
      "VERB is be\n",
      "PRON us -PRON-\n",
      "NOUN yesterdays yesterday\n",
      "NOUN schools school\n",
      "NOUN tomorrows tomorrow\n",
      "PRON us -PRON-\n",
      "VERB is be\n",
      "ADP as as\n",
      "ADP as as\n",
      "PRON us -PRON-\n",
      "NOUN basis basis\n",
      "NOUN basis basis\n",
      "PRON us -PRON-\n",
      "PRON us -PRON-\n",
      "VERB houses house\n",
      "DET his -PRON-\n",
      "PRON us -PRON-\n",
      "NOUN waters water\n",
      "NOUN righteousness righteousness\n",
      "PRON us -PRON-\n",
      "DET his -PRON-\n",
      "PRON us -PRON-\n",
      "PRON us -PRON-\n",
      "PART 's 's\n",
      "VERB confess confess\n",
      "NOUN friends friend\n",
      "ADV always always\n",
      "NOUN places place\n",
      "NOUN points point\n",
      "NOUN setbacks setback\n",
      "NOUN moments moment\n",
      "NOUN dreams dream\n",
      "ADV sometimes sometimes\n",
      "NOUN hopes hope\n",
      "NOUN eyes eye\n",
      "ADJ courageous courageous\n",
      "NOUN rights right\n",
      "NOUN acts act\n",
      "NOUN mobs mob\n",
      "ADP as as\n",
      "VERB is be\n",
      "NOUN days day\n",
      "ADJ audacious audacious\n",
      "ADP as as\n",
      "NOUN words word\n",
      "VERB was be\n",
      "PROPN james James\n",
      "NOUN days day\n",
      "NOUN fathers father\n",
      "NOUN tears tear\n",
      "NOUN paths path\n",
      "VERB is be\n",
      "DET this this\n",
      "PRON us -PRON-\n",
      "NOUN uncertainties uncertainty\n",
      "ADP as as\n",
      "NOUN days day\n",
      "NOUN clouds cloud\n",
      "NOUN nights night\n",
      "NOUN midnights midnight\n",
      "PRON us -PRON-\n",
      "VERB is be\n",
      "DET this this\n",
      "NOUN mountains mountain\n",
      "VERB is be\n",
      "NOUN yesterdays yesterday\n",
      "NOUN tomorrows tomorrow\n",
      "PRON us -PRON-\n",
      "VERB is be\n",
      "VERB bends bend\n",
      "PRON us -PRON-\n",
      "VERB is be\n",
      "PRON us -PRON-\n",
      "VERB is be\n",
      "VERB is be\n",
      "DET this this\n",
      "VERB is be\n",
      "DET this this\n"
     ]
    }
   ],
   "source": [
    "# return the POS tag, word in lowercase, and the lemmatized word\n",
    "# for the words in the second doc. \n",
    "for word in docs[1]:\n",
    "    if word.text.endswith('s'):\n",
    "        print(word.pos_, word.lower_, word.lemma_) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>There are many different NLP toolkits and no library is perfect. It looks like spaCy misattributed the POS tag for some words due to case usage and this caused the lemmatizer to miss them. </h4> \n",
    "\n",
    "For performance benchmarks on how spaCy compares to other libraries, check out: https://spacy.io/usage/facts-figures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Now we are going to reload corpus again.</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "docs = []\n",
    "for filename in glob.glob('./King/*.txt'):  \n",
    "    with open(filename, 'r', encoding='utf-8') as f:     \n",
    "        docs.append(f.read().replace('\\n', ' '))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Next we are going to preprocess with Stanford CoreNLP --a Java library with a Python wrapper which provides support for several world languages.</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use device: cpu\n",
      "---\n",
      "Loading: tokenize\n",
      "With settings: \n",
      "{'model_path': '/Users/ppchsdbib/stanfordnlp_resources/en_ewt_models/en_ewt_tokenizer.pt', 'lang': 'en', 'shorthand': 'en_ewt', 'mode': 'predict'}\n",
      "---\n",
      "Loading: lemma\n",
      "With settings: \n",
      "{'model_path': '/Users/ppchsdbib/stanfordnlp_resources/en_ewt_models/en_ewt_lemmatizer.pt', 'lang': 'en', 'shorthand': 'en_ewt', 'mode': 'predict'}\n",
      "Building an attentional Seq2Seq model...\n",
      "Using a Bi-LSTM encoder\n",
      "Using soft attention for LSTM.\n",
      "Finetune all embeddings.\n",
      "[Running seq2seq lemmatizer with edit classifier]\n",
      "---\n",
      "Loading: pos\n",
      "With settings: \n",
      "{'model_path': '/Users/ppchsdbib/stanfordnlp_resources/en_ewt_models/en_ewt_tagger.pt', 'pretrain_path': '/Users/ppchsdbib/stanfordnlp_resources/en_ewt_models/en_ewt.pretrain.pt', 'lang': 'en', 'shorthand': 'en_ewt', 'mode': 'predict'}\n",
      "Done loading processors!\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "import stanfordnlp\n",
    "nlp = stanfordnlp.Pipeline(processors =\"tokenize,lemma,pos\") \n",
    "\n",
    "# stanfordnlp doesn't come with its own stop words list\n",
    "# so we are going to have to import one from NLTK again. \n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english')) \n",
    "\n",
    "cleanDocs = []\n",
    "\n",
    "def stanford_preprocess(corpus): \n",
    "    for doc in range(len(corpus)):\n",
    "        corpus[doc] = nlp(corpus[doc])\n",
    "        (cleanDocs.append([words.lemma.lower() for sent in corpus[doc].sentences for words\n",
    "                           in sent.words if words.lemma.lower() not in stop_words and\n",
    "                           words.lemma.isalpha()]))\n",
    "    #return cleanDocs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "stanford_preprocess(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Let's take a look at the third document.</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['want', 'use', 'subject', 'preach', 'three', 'dimension', 'complete', 'life', 'right', 'know', 'use', 'tell', 'hollywood', 'order', 'movie', 'complete', 'three', 'dimensional', 'well', 'morning', 'want', 'seek', 'get', 'life', 'complete', 'yes', 'must', 'three', 'dimensional', 'many', 'many', 'century', 'ago', 'man', 'name', 'john', 'find', 'prison', 'lonely', 'obscure', 'island', 'call', 'patmos', 'right', 'right', 'prison', 'enough', 'know', 'lonely', 'experience', 'right', 'incarcerate', 'situation', 'deprive', 'almost', 'every', 'freedom', 'freedom', 'think', 'freedom', 'pray', 'freedom', 'reflect', 'meditate', 'john', 'lonely', 'island', 'prison', 'right', 'lift', 'vision', 'high', 'heaven', 'right', 'see', 'descend', 'heaven', 'new', 'heaven', 'right', 'new', 'earth', 'right', 'twenty', 'first', 'chapter', 'book', 'revelation', 'open', 'say', 'see', 'new', 'heaven', 'new', 'earth', 'right', 'john', 'see', 'holy', 'city']\n"
     ]
    }
   ],
   "source": [
    "print(cleanDocs[2][:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Now reload the corpus one more time.</h4>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "docs = []\n",
    "for filename in glob.glob('./King/*.txt'):  \n",
    "    with open(filename, 'r', encoding='utf-8') as f:     \n",
    "        docs.append(f.read().replace('\\n', ' '))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Now we are going to preprocess with Spark NLP --an enterprise-scale library built on Apache Spark designed for all your big data needs.</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "explain_document_dl download started this may take some time.\n",
      "Approx size to download 167.3 MB\n",
      "[OK!]\n"
     ]
    }
   ],
   "source": [
    "# getting started wth spark and sparknlp \n",
    "import sparknlp \n",
    "from sparknlp.pretrained import PretrainedPipeline\n",
    "spark = sparknlp.start()\n",
    "pipeline = PretrainedPipeline('explain_document_dl', lang='en') \n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english')) \n",
    "\n",
    "cleanDocs = []\n",
    "\n",
    "def sparknlp_preprocess(corpus): \n",
    "     for doc in corpus:\n",
    "        doc = pipeline.annotate(doc) \n",
    "        (cleanDocs.append([word.lower() for word in doc['lemma'] if word.lower() \n",
    "                           not in stop_words and word.isalpha()]))\n",
    "        #return cleanDocs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "sparknlp_preprocess(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>As the last step, let's get a snapshot of the contents of the forth document.</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['force', 'preach', 'something', 'handicap', 'morning', 'fact', 'doctor', 'come', 'church', 'say', 'would', 'good', 'stay', 'bed', 'morning', 'insist', 'would', 'come', 'preach', 'allow', 'come', 'one', 'stipulation', 'would', 'come', 'pulpit', 'time', 'preach', 'would', 'immediately', 'go', 'back', 'home', 'get', 'bed', 'go', 'try', 'follow', 'instruction', 'point', 'want', 'use', 'subject', 'preach', 'morning', 'familiar', 'subject', 'familiar', 'preach', 'subject', 'twice', 'know', 'pulpit', 'try', 'make', 'something', 'custom', 'tradition', 'preach', 'passage', 'scripture', 'least', 'year', 'add', 'new', 'insight', 'develop', 'along', 'way', 'new', 'experience', 'give', 'message', 'although', 'content', 'basic', 'content', 'new', 'insight', 'new', 'experience', 'naturally', 'make', 'new', 'illustration', 'want', 'turn', 'attention', 'subject', 'loving', 'enemies', 'itys', 'basic', 'part', 'basic', 'philosophical', 'theological', 'orientation', 'whole', 'idea']\n"
     ]
    }
   ],
   "source": [
    "print(cleanDocs[3][:100])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
